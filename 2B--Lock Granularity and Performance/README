Project 2B: Lock Granularity and Performance

NAME: Jingjing 
EMAIL: 
ID: 


Included Files
==============
SortedList.h
	The header file containing interfaces for linked list operations.

SortedList.c
	The source for a C source module that compiles cleanly and implements insert, 
	delete, lookup, and length methods for a sorted doubly linked list (described 
	in the provided header file, including correct placement of pthread_yield calls).

lab2_list.c
	The source for a C program that compiles cleanly and implements the specified 
	command line options (--threads, --iterations, --yield, --sync, --lists), drives 
	one or more parallel threads that do operations on a shared linked list, and 
	reports on the final list and performance. 

lab2b_list.csv
	The file that contains the results for all of test runs.

profile.out 
	The execution profiling report showing where time was spent in the un-partitioned 
	spin-lock implementation.

graphs 
	The .png files that are created by gnuplot(1) on the above csv data showing:

	lab2b_1.png 
		throughput vs. number of threads for mutex and spin-lock synchronized list operations.
	lab2b_2.png 
		mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
	lab2b_3.png 
		successful iterations vs. threads for each synchronization method.
	lab2b_4.png 
		throughput vs. number of threads for mutex synchronized partitioned lists.
	lab2b_5.png 
		throughput vs. number of threads for spin-lock-synchronized partitioned lists.

Makefile
	The file to build the deliverable programs, output, graphs, and tarball.

	tests
		run all specified test cases to generate CSV results
	profile 
		run tests with profiling tools to generate an execution profiling report
	graphs
		use gnuplot to generate the required graphs
	dist 
		create the deliverable tarball
	clean 
		delete all programs and output generated by the Makefile

README
	The file that contains descriptions of each of the included files and any other 
	related information (e.g. research, limitations, features, testing methodology).
	It also contains brief (a few sentences per question) answers to each of the 
	questions.


Research
========
gperftools: https://gperftools.github.io/gperftools/cpuprofile.html
			https://github.com/gperftools/gperftools


Questions
=========
QUESTION 2.3.1 - CPU time in the basic list implementation:
	In the 1 and 2-thread list tests, since there are only a small number of threads, 
	most of the CPU time will not be spent on the locking and waiting mechanisms as 
	there won't be a lot of threads waiting. Instead, the time will mostly be spent on 
	performing the list operations.
	In the high-thread spin-lock tests, since there are now a lot of threads waiting
	due to the spin-lock mechanism, most of the CPU time will be spent on threads waiting
	for the locks to be released.
	In the high-thread mutext tests, since the mutex operations are costly, if there are
	a lot of locks, the overall cost will be higher. So most of the CPU time will be spent
	on the mutex operations.

QUESTION 2.3.2 - Execution Profiling:
	According to the output of profiling, the lines that are consuming the most of the CPU
	time when the spin-lock version of the list exerciser is run with a large number of 
	threads are the parts working on acquiring the lock. This operation becomes so expensive 
	with large numbers of threads becauses as more threads are competing for the critical 
	section, where only one thread can enter, more threads will be keeping spinning and waiting. 
	Therefore, a lot of cycles are wasted, and the costs are high.

QUESTION 2.3.3 - Mutex Wait Time:
	The average lock-wait time rises so dramatically with the number of contending threads
	because although there are more contending threads, still only one thread can access the
	critical section. As a result, the number of threads that are waiting to enter the critical
	section will rise as well. So more time will be spent on waiting. 
	The reason why the completion time per operation rises less dramatically with the number 
	of contending threads is that the time of completion is not affected by the length of the 
	waiting time. No matter how long each thread will have to wait for the critical section, 
	there is always one thread performing the critical section.
	It is possible for the wait time per operation to go up faster (or higher) than the completion 
	time per operation. The reason is that while the completion time is measured by the parent, 
	the wait time per operation is measured by each thread, and thus will include the overlaped 
	wait time in which more than one thread are waiting at the same time. So the sum of the wait
	time will include those overlapses and thus be greater than the compeletion time.

QUESTION 2.3.4 - Performance of Partitioned Lists
	The performance of the synchronized methods improves as the number of lists increases. If the
	number of lists continues to increase, the throughput will eventually reach a constant stage
	where the number of thread and the number of list match with each other. At this stage, since
	each thread can now have its own list, adding more lists will no longer benefit the throughput.
